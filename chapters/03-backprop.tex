\section{Feed Forward Networks and Back-propagation}
\q{MLP: Why is it challenging to add hundreds of hidden layers?}

\q{On the above graph, consider x and y as input terms, g as
intermediate term, z as model term and f as output term. Which
terms will not be updated?}

\q{How do we make use of the gradients to update the network
parameters?}

\q{Does back-propagation apply to unsupervised learning too?}

\q{What is missing from the PyToch implementation, compared to
python code from above?}

\q{Do we compute the Jacobian for the loss L as well?}

\q{Name applications that require
the identity module as output.}

\q{What is the motivation behind
ReLU?}

\q{Is Leaky ReLU a good
activation for regression?}

\q{Is ELU a good activation for
regression?}

\q{Sigmoid: What is the advantage by
bounding the range of the
function?}

\q{Tanh: What is the main difference
from sigmoid?}

\q{Softmax + Autograd: Do we need to compute the
complete Jacobian matrix?}

\q{Softmax numerical stability: What could happen if we donâ€™t shift the values to be negative?}


