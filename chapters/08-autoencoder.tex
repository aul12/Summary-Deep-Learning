\section{Autoencoders}

\q{Why do we reduce the dimensions of the data?}

\q{How can we reduce the dimension of the data?}

\q{How does an autoencoder look like? Illustrate it.}

\q{What would be a suitable loss function for an autoencoder?}

\q{The latent code is a vector or tensor of much lower dimensions than
the input image. Which type of layers are required in the decoder to
reconstruct the input?}

\q{Principal Component Analysis (PCA) is a linear transformation for
reducing the data dimensions. What is the main advantage of an
autoencoder compared to PCA?}

\q{What is the difference between supervised and unsupervised training?}

\q{k-Sparse-AE: How can we choose how many activations to completely cancel?}

\q{k-Sparse-AE: The algorithm works using a single hidden layer. How can we train
deep neural networks on the same way?}

\q{Denoising-AE: Can we use the KL divergence as the loss function? Explain the
reason of choice.}

\q{How can we quantitatively evaluate the denoising autoencoder? Our
goal is to examine how useful the latent code is.}

\q{How do we train a stacked autoencoder?}

\q{In the autoencoders, we have seen the idea of progressive training.
How does this differ from standard supervised learning?}

\q{Would it be useful to have skip connections from the decoder to
encoder?}

\q{What are the common operations to reduce the dimensions?}

\q{Which popular deep architecture for image recognition makes use of
skip connections with a different name?}

