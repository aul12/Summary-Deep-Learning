\section{Recurrent Neural Networks}
\q{Name a few applications of sequential data.}

\q{What are the main challenges of sequential modeling with recurrent
neural networks compared to single sample processing with
feed-forward neural networks?}

\q{How do we represent the words so they are compatible with
recurrent neural networks?}

\q{How do we represent the ground truth of a multiple class recognition
problem?}

\q{How does the RNN look like?}

\q{This RNN memory representation is
generic but less intuitive. Can we
represent the RNN in layer-wise
manner (similar to ConvNets)?}

\q{If the unfolded RNN looks like a normal feed-forward network, can
we also train it like a feed-forward network?}

\q{What are the main differences from feed-forward neural networks?}

\q{What is the main difference between RNN and feed-forward network
parameter updates?}

\q{Which factors affect the RNN structure?}

\q{What is the problem with vanishing gradients if itâ€™s still possible to
update the parameters?}

\q{What is the LSTM gate?}
