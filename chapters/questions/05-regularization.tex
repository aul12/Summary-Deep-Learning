\section{Regularization, Parameter Initialization and Normalization}

\q{Is label augmentation a useful technique for classification problems?}

\q{What is co-adaptation?}

\q{What initialization strategies have we seen up to now?}

\q{Which figure represents the the Glorot initialization? (Slide 41)}

\q{What is the limitation of the
Glorot initialization when the
network has ReLU activations?}

\q{Rescaling: How?}

\q{What could be one disadvantage of the unit length normalization?}

\q{How can we reduce the data dimensions?}

\q{Batch-Normalization: During training we use the mini-batch information for computing the
mean and variance. What can we do at the test time?}

\q{Can the small mini batch size be helpful for batch normalization?}

