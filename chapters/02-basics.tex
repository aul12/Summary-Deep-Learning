\section{Machine Learning Basics}

\q{Classification: Define the mapping function $f$ for probabilistic output}

\q{Regression: What about high dimensional output? Define the mapping function.}

\q{What is the main disadvantage of supervised learning?}

\q{Linear Regression: How do we minimize the error?}

\q{Linear Regression: What are the values that correspond to the best parameters?}

\q{Linear Regression (closed form solution):
What happens when we significantly increase the number of samples
and features?}

\q{How is the model capacity connected to under-fitting and
over-fitting? Can you graphically illustrate these two problems?}

\q{Over- /Undefitting: Is there any other way to regulate these two problems?}

\q{Regularization: Is there any other way to further reduce the generalization error?}

\q{What is the Bayesâ€™ theorem?}

\q{How do we measure the dissimilarity, or similarity, between two
distributions?}

\q{KL-Divergence: How would it look like for continuous probability distributions?}

\q{KL-Divergence ($D_\text{KL}(P||Q)$): What is the reference distribution in our problem?}

\q{Why do we call it divergence and not distance?}

\q{Gradient-Descent: How do we compute the gradients? What are the advantages and
disadvantages in every case?}

\q{Illustrate the limitation of linear regression to solve a two-class
classification problem. Then draw the logistic regression to show
how it separates the data into two categories.}
