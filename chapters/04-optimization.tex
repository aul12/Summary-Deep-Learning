\section{Optimization}

\q{What is on factor that makes a neural network memorize?}

\q{How do we observe the convergence?}

\q{What are the variants of gradient descent?}

\q{SGX with Momentum: How do you interpret the velocity when $\alpha = 0.9$}

\q{Adagrad: How can we measure the update frequency for each parameter?}

\q{What is the limitation of Adagrad?}

\q{How do we call the matrix for second order derivatives?}

\q{Adam: Why is the bias important?}

\q{Adam: How do we derive the bias-corrected estimates?}

\q{Adam: What is the value of $(1-\beta_1) \sum_{i=1}^t \beta_1^{t-i}$ after a few thousands of
iterations?}


